{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn as sk\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport itertools\nsns.set_theme(style=\"whitegrid\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Importing the training data\n\nWe import the titanic training data from `train.csv`."},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT = '/kaggle/input/'\nDATADIR = 'titanic/'\n\ntitanic = pd.read_csv(ROOT+DATADIR+'train.csv')\ntitanic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive analysis\n\nWe should examine our data for a moment and get an idea of its structure. We have 12 features and , with a mix of numerical and categorical data. If we want to perform any kind of numerical analysis involving the categorical features, we will probably need to transform it using e.g. a label encoder. \n\nWe must also account for missing features in a record. We can use an imputer to fill NaN values with some value that we hope will not influence the statistics of the dataset; our filling strategy depends on how are data is distributed in that feature (skewed left, skewed right, centered on mean value, long tail, etc). Note that if the feature is categorical, we must convert the feature to numeric before we can impute any NaN values (if we wish to define statistical properties for that feature)."},{"metadata":{},"cell_type":"markdown","source":"Let's get the descriptive statistics about the numerical data. We will ignore the categorical data for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 891 observations and 7 numerical features. \n- `PassengerId` contains a unique identifier for each passenger on Titanic. \n- `Survived` contains the information that we wish to predict using some model of our choice or design. According to this dataset, only 38% of Titanic passengers survived the sinking. \n- `Pclass` is the ticket class. \n- `SibSp` is the number of siblings+spouses aboard the Titanic, for each unique PassengerId (each passenger has some number of siblings and/or a spouse with them) \n- `Age` is age\n- `Parch` is the number of children+parents aboard the Titanic, for each unique PassengerId (similar to `SibSp`)\n- `Fare` is the ticket cost (assume the currency is standardized, considering that the Titanic embarked from ports in France, England, and New Zealand)"},{"metadata":{},"cell_type":"markdown","source":"### Cleaning the data\n\nWe notice that `Age` is missing 177 entries to the descriptive statistics, which suggests that our dataset may contain null or missing information. Let's first get an idea of exactly how much information is missing from all columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that `Cabin` contains 687 null values and `Embarked` contains 2 null values. Although cabin might be an interesting feature to study related to Titanic survivably (perhaps passengers in cabins closest to the escape boats were more likely to survive?), let's elminate it from our discussions in this report since we are losing over half of the `Cabin` information to null values.\n\nWe might hypothesize that `Age` plays a non-neglible role in Titanic survivability; therefore, rather than come up with a fill strategy, since `Age` may be an important predictor, we would be better off removing these rows completely from our training steps.\n\nWe could replace the 2 null values in `Embarked` with a fourth category, such as U for Unknown."},{"metadata":{},"cell_type":"markdown","source":"Let's drop the rows with null `Age` values, drop the `Cabin` column, and perform the suggested replacement in `Embarked`."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_dropped_na = titanic.dropna(subset=['Age']).drop(columns=['Cabin'])\ntitanic_dropped_na['Embarked'] = titanic_dropped_na['Embarked'].fillna('U')\ntitanic_dropped_na.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all null values have been eliminated from the dataset."},{"metadata":{},"cell_type":"markdown","source":"### Feature analysis\n\nNext, we wish to examine the distributions of each feature. We can plot bar charts (histograms) of categorical feature, and use a kernel density estimator to approximately plot the distribution in each numerical feature. We omit the `Name` and `Ticket` columns from these observations, because these can be tied into the unique identifier provided by `PassengerId`."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ncategoric_cols = ['Sex', 'Embarked']\nrelevant_cols = numerical_cols + categoric_cols\n\n# ------- FOR SEABORN 0.10.0 ------------------------------------------\n# fig, axes = plt.subplots(3, 3, figsize=(16,8))\n# fig.tight_layout(h_pad=5, w_pad=5)\n# axes_points = list(set(itertools.combinations([0,1,2,0,1,2],2)))\n\n# for i in range(len(numerical_cols)):\n#     column = numerical_cols[i]\n#     ax = axes_points[i]\n#     sns.displot(data=titanic_dropped_na, x=column, ax=axes[ax], kde=True)\n    \n# fig, axes = plt.subplots(1, 2, figsize=(8,4))\n# fig.tight_layout(h_pad=5, w_pad=5)\n\n# for i in range(len(categoric_cols)):\n#     column = categoric_cols[i]\n#     ax = axes_points[i]\n#     sns.countplot(x=column, data=titanic_dropped_na, ax=axes[i])\n# ----------------------------------------------------------------------\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16,8))\nfig.tight_layout(h_pad=5, w_pad=5)\nfor i, column in enumerate(relevant_cols):\n    sns.histplot(titanic_dropped_na[column],ax=axes[i//3,i%3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventually, we will probably want these numerical data scaled and normalized. Many prediction models, such as SVM classifiers, work best when using data transformed in this way. If we wish to use the categorical data for any sort of quantitative analyses, we will need to use some sort of encoder to transform the string variables in continuous variables.\n\nWe will continue this discussion in the exploratory analysis section below."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Exploratory analysis\n\nThe goal is to predict which passengers survive. We need to assess which variables in our dataset are most correlated with the target variable, i.e. the column `Survived`. We can visualize correlations in our dataset by 1) obtaining the correlation matrix of the entire dataset and then 2) plotting the correlation matrix as a heatmap. Pandas and Seaborn have tools that let us complete these tasks.\n\nSmall aside on the correlation matrix. The correlation between two random variables $X$ and $Y$, as measured by the Pearson correlation coefficient, can be written as:\n\n$$\\text{corr}(X,Y) = \\frac{E[XY] - E[X]E[Y]}{\\sigma_X \\sigma_Y}$$\n\nwhere $E[X]$ is the *expected value* of $X$ and $\\sigma_X$ is the standard deviation. Note that $\\sigma_X = \\sqrt{E[X^2] - E[X]^2}$. In a practical sense, the *expected value* is equivalent to the arithmetic mean. Then, the Pearson correleation coefficient measures the amount of *linear* dependence between the two random varibles $X$ and $Y$. A value of $\\text{abs}\\left(\\text{corr}(X,Y)\\right) \\rightarrow 1$ implies that variation in the product $XY$ can be explained by the product of their variations, $\\sigma_X \\sigma_Y$. The sign implies the direction of the linear relationship (positive/negative slope). For $N$ random variables $X_1, X_2, \\dots X_N$, we can construct a *correlation matrix* $\\mathbf{M}$, with elements $\\mathbf{M}_{ij} = \\text{corr}(X_i,X_j)$, to visualize correlations between each pair of random variables. Of course, we can summarily ignore the diagonal elements, since we already know each random variable will be correlated with itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_corr = titanic_dropped_na.corr() # get the correlation matrix\n\nfig = plt.figure(figsize=(16,8))\nsns.heatmap(titanic_corr, annot=True) # plot the correlation matrix as a heatmap, annotating each cell with the correlation coefficient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We examine the correlation matrix for dependence of survival on our predictors, which are PassengerId, Pclass, Age, SibSp, Parch, and Fare. Pclass and Fare are the most correlated with `Survived`, so we may choose to treat these variables with importance."},{"metadata":{},"cell_type":"markdown","source":"Out of curiousity, let's perform a one-hot encoding on the categorical data, so that we might see how port of embarkation might affect survivability."},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_embark = pd.get_dummies(titanic_dropped_na.Embarked, prefix = 'Embarked')\ntitanic_dropped_na_join = titanic_dropped_na.join(one_hot_embark)\ntitanic_dropped_na_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Embarked_C', 'Embarked_Q', 'Embarked_S', 'Embarked_U']\nports = ['Cherbourg', 'Queenstown', 'Southampton', 'Unknown']\n\ntotal_passengers = titanic_dropped_na_join.shape[0]\nfor i, name in enumerate(ports):\n    total_pass_from_port = titanic_dropped_na_join[columns[i]].sum()\n    perc_pass_from_port = total_pass_from_port/total_passengers*100\n\n    print(\"{:3d} passengers ({:5.2f}% of total passengers) embarked from {:s}\".format(total_pass_from_port, perc_pass_from_port, name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we can take another look at the correlation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_corr = titanic_dropped_na_join.corr() # get the correlation matrix\n\nfig = plt.figure(figsize=(16,8))\nsns.heatmap(titanic_corr, annot=True) # plot the correlation matrix as a heatmap, annotating each cell with the correlation coefficient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Embarked_C` and `Embarked_S` are almost as correlated with `Survived` as `Fare`. There are only two data points present in `Embarked_U`, so we cannot say much about its relationship to `Survived`. Let's also look at how many passengers survived from each port of embarkation. This may explain some of the correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Embarked_C', 'Embarked_Q', 'Embarked_S', 'Embarked_U']\nports = ['Cherbourg', 'Queenstown', 'Southampton', 'Unknown']\n\ntotal_survived = titanic_dropped_na_join.Survived.sum()\nfor i, name in enumerate(ports):\n    total_surv_from_port = titanic_dropped_na_join[titanic_dropped_na_join['Survived'] > 0][columns[i]].sum()\n    perc_surv_from_port = total_surv_from_port/total_survived*100\n\n    print(\"{:3d} survivors ({:5.2f}% of total survivors) embarked from {:s}\".format(total_surv_from_port, perc_surv_from_port, name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the correlation makes sense. The largest proportion of passengers embarked from Southampton, so, of course, the largest proportion of survivors embarked from Southampton. The proportion of survivors from each port also seems to resemble the proportion of passengers each port (within a few percent), which suggests that, within a given port of embarkation, a passenger was no more or less likely to survive than a passenger from a different port of embarkation. That is, each passenger had an equal chance to survive based on the port they embarked from. However, from a purely prediction standpoint, picking a passenger at random from the population, we could say that `Embarked` is a reasonable predictor for survivability. We will include the one-hot embarked in our prediction models."},{"metadata":{},"cell_type":"markdown","source":"Let's plot our data on a plane to see if we can observe any obvious clustering that might affect which models we decide to train in order to make predictions. We can plot our two most promising features, `Pclass` and `Fare`, against each other, with `Survived` on a color axis, to gain some qualitative insight on how easily we can find a separation plane between \"survived\" and \"did not survive\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=titanic_dropped_na_join, x='Pclass', y='Fare', hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually, we run into some problems here because `Pclass` is not technically a continuous variable; it is a label that represents the socio-economic \"class\" of the passenger for which the ticket was purchased; however, a simple analogy would be to compare `Pclass` to \"business\" and \"coach\" on a plane (ignoring the fact that `Fare` would not vary so significantly in this case, since in plane ticket class prices are fixed). We do gain some qualitative insight from this strip plot: as far as clustering goes, the largest number of survivors is clustered in the 1st class ticket category. This observation motivates our use of `Pclass` as a predictor in our models.\n\nLet's also examine `Fare` vs `Age` for an example of how we might analyze clustering for two continuous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=titanic_dropped_na_join, x='Fare', y='Age', hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Age` is one of our weaker predictors for `Survived`, and we can see why. When we plot `Age` in a plane with `Fare` (one of our stronger predictors), no clear separation boundary is obvious between the two classes of `Survived`."},{"metadata":{},"cell_type":"markdown","source":"# Predictive analysis \n \nNext we will construct several models for predicting survivablility, and choose which one performs with the highest test case accuracy. We can build a model-testing pipeline using tools provided by the `sklearn` package.\n\nHere we list the models we intend to test. Generally speaking, we can present the problem of predicting survivability as a classification problem. The Survived column contains binary (categorical) data, i.e. 1 if the passenger survived and 0 if the passenger did not survive. `sklearn` implements several classifiers. We understand that the goal of every classifier is to find a hyperplane that separates our data into \"survived\" and \"did not survive\".\n\nModels:\n1. SVC\n    - SVMs attempt to find a hyperplane that separates categorical data by, essentially, maximizing the width between two categories of data\n1. KNeighborsClassifier\n    - \"Majority voting\" scheme where each data point is classified based on the labels of the $k$-nearest neighbors in a neighborhood around that point\n1. RandomForestClassifier\n    - A \"forest\" of decision tree classifiers. In a decision tree classifier, each \"branch\" is a feature that modulates the probability that a certain classification is made; e.g. a flower with petal width greater than 5 cm is 70% likely to be class 1, and less than 5 cm is 30% likely to be class 2. These probabilities are tuned by exposing the tree (or forest) to many examples during training\n1. MLPClassifier\n    - Neural network classifier. Several layers of nodes (input, hidden, and output) are linked together by weights, which are tuned through backpropagation according to some cost function\n    \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nX_train = titanic_dropped_na_join[['Pclass', 'Fare', 'Embarked_C', 'Embarked_S']]\n\ny_train = np.ravel(titanic_dropped_na_join[['Survived']])\n\n\nclassifiers = [MLPClassifier(), SVC(), RandomForestClassifier(), KNeighborsClassifier()]\n\nfor classifier in classifiers:\n    model_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('classifier', classifier)])\n    model_pipe.fit(X_train, y_train)\n    print(classifier)\n    print('model training score: {:0.5f}\\n'.format(model_pipe.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Naively training these classifiers with default arguments, we see that some classifiers do achieve reasonable accuracy. The \"model score\" is the cross-validated mean squared error of the predicted output."},{"metadata":{},"cell_type":"markdown","source":"This dataset is small enough that we might as well try to train our models with all the features we have available. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all irrelevant categorical data\nX_train = titanic_dropped_na_join.drop(columns=['Survived', 'Name', 'Ticket', 'Embarked', 'Sex'])\n\ny_train = np.ravel(titanic_dropped_na_join[['Survived']])\n\n\nclassifiers = [MLPClassifier(), SVC(), RandomForestClassifier(), KNeighborsClassifier()]\n\nfor classifier in classifiers:\n    model_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('classifier', classifier)])\n    model_pipe.fit(X_train, y_train)\n    print(classifier)\n    print('model training score: {:0.5f}\\n'.format(model_pipe.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Generally, performance increases for all classifiers after including all features. The RandomForestClassifier performs best, but we should be cautious of perfect predictions; usually, this means the model will not generalize well to an isolated testing set (overfitting). We could one-hot encode `Sex` to see if that feature offers another performance increase.\n\nOur next task will be to perform a GridSearch for each model, in an attempt to optimize the model parameters and squeeze out the best performance from each one. We also wish to find under which conditions each model performs the best on a *testing* set (using cross validation)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# drop all irrelevant categorical data\nX_train = titanic_dropped_na_join.drop(columns=['Survived', 'Name', 'Ticket', 'Embarked', 'Sex'])\n# X_train = titanic_dropped_na_join[['Pclass', 'Fare', 'Embarked_C', 'Embarked_S']]\ny_train = np.ravel(titanic_dropped_na_join[['Survived']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Coarse grid search for each classifier\n\nWe execute a coarse grid search on a custom parameter grid for each classifier we wish to tune for performance. This grid search is intended to be a cursory \"first pass\" to find which parameter combinations are worth tuning in more detail, in order to gain the most accuracy from a particular model. The cross-validated score represents the mean accuracy achieved on five folds of cross validation. Each fold isolates 1/5 of the training data as a testing set (i.e. data the trained model has never seen) and trains on the remaining 4/5 of the data."},{"metadata":{},"cell_type":"markdown","source":"### MLPClassifier\n\nGridSearch on MLPClassifier. Parameters tested:\n\n- `hidden_layer_sizes`: shape of hidden layers in neural network\n- `activation`: activation function for the hidden layer\n- `solver`: weight optimization\n- `alpha`: L2 regularization strength\n- `learning_rate`: method to determine weight updates"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmlp_class = MLPClassifier(max_iter=500)\nmodel_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('MLPClassifier', mlp_class)])\nprint(model_pipe.get_params().keys())\nparam_grid = {'MLPClassifier__hidden_layer_sizes' : [(10,), (10,10), (100,)],\n              'MLPClassifier__activation' : ['logistic', 'tanh', 'relu'],\n              'MLPClassifier__solver' : ['adam', 'sgd'],\n              'MLPClassifier__alpha' : [1e-7, 1e-5,1e-1, 1],\n              'MLPClassifier__learning_rate' : ['constant']\n             }\n\nsearch = GridSearchCV(model_pipe, param_grid, n_jobs=-1, verbose=2)\nsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_search_data = pd.DataFrame(search.cv_results_)\nmlp_search_data.sort_values('rank_test_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### SVC\n\nGridSearch on Support Vector Classifier. Parameters tested:\n\n- `C`: regularization strength (inverse to magnitude; squared L2 regularization)\n- `kernel`: the kernel function defines \"distance measure\" between points in parameter space (remember, goal is to find greatest separation in data)\n- `degree`: polynomial kernel function degree\n- `gamma`:  normalization for RBF, poly, and sigmoid kernel functions\n- `coef0`: constant term in poly and sigmoid kernel funtions0"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_class = SVC(tol=1e-10)\nmodel_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('SVC', svc_class)])\nprint(model_pipe.get_params().keys())\nparam_grid = {'SVC__C' : [1e-3,1e-1, 1.0, 10, 100],\n              'SVC__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n              'SVC__coef0' : [-1, -1e-2, 0.0, 1e-2, 1],\n              'SVC__degree' : [2,3,4],\n              'SVC__gamma' : ['scale', 'auto']\n             }\n\nsearch = GridSearchCV(model_pipe, param_grid, n_jobs=-1, verbose=2)\nsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_search_data = pd.DataFrame(search.cv_results_)\nsvc_search_data.sort_values('rank_test_score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier\n\nGridSearch on RandomForestClassifier. This is a complicated classifier with many parameters to test, dealing with tree depth, pruning thresholds, branching probabilities, etc. Parameters tested:\n\n- `n_estimators`: number of decision trees\n- `max_depth`: maximum depth of each tree in the forest\n- `min_samples_split`: minimum number of samples required to split a node\n- `max_features`: number of features for best split; each tree is randomly assigned this number of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_class = RandomForestClassifier()\nmodel_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('RandomForestClassifier', rfc_class)])\nprint(model_pipe.get_params().keys())\nparam_grid = {'RandomForestClassifier__n_estimators' : [10, 100, 1000],\n              'RandomForestClassifier__max_depth' : [10, 50, 100, 1000],\n              'RandomForestClassifier__min_samples_split':[2,3,10],\n              'RandomForestClassifier__max_features' : ['auto', 'sqrt', 'log2', 0.5*X_train.shape[1]]\n             }\n\nsearch = GridSearchCV(model_pipe, param_grid, n_jobs=-1, verbose=2)\nsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_search_data = pd.DataFrame(search.cv_results_)\nrfc_search_data.sort_values('rank_test_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### KNeighborsClassifier\n\nGridSearch on k-nearest neighbors classifier. Parameters tested:\n\n- `n_neighbors`: number of neighbors to compare against\n- `weights`: weight function\n- `p`:  changes dimension of metric (default metric is Minkowski)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knc_class = KNeighborsClassifier()\nmodel_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), ('KNeighborsClassifier', knc_class)])\nprint(model_pipe.get_params().keys())\nparam_grid = {'KNeighborsClassifier__n_neighbors' : [2, 3, 5, 8, 10],\n              'KNeighborsClassifier__weights' : ['uniform', 'distance'],\n              'KNeighborsClassifier__p' : [1,2,3,4,5]\n             }\n\nsearch = GridSearchCV(model_pipe, param_grid, n_jobs=-1, verbose=2)\nsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knc_search_data = pd.DataFrame(search.cv_results_)\nknc_search_data.sort_values('rank_test_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp = mlp_search_data.sort_values('rank_test_score').reset_index()\nsvc = svc_search_data.sort_values('rank_test_score').reset_index()\nrfc = rfc_search_data.sort_values('rank_test_score').reset_index()\nknc = knc_search_data.sort_values('rank_test_score').reset_index()\n\n\nbest_scores = pd.DataFrame({'classifier' : ['MultilayerPerceptron','SupportVectorClassifier','RandomForestClassifier','KNeighborsClassifier'],\n                            'best_mean_score' : [mlp['mean_test_score'][0], svc['mean_test_score'][0], rfc['mean_test_score'][0],knc['mean_test_score'][0]]\n                           })\n\nfig = plt.figure(figsize=(16,8))\nsns.barplot(y='classifier', x='best_mean_score', data=best_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average cross-validated scores for each classifier tested are extremely similar, with each score approaching 75% accuracy. The fact that each classifier is so similar suggests we are feature-limited. We may need to perform some feature engineering to add additional features, and hopefully increase the performance of each classifier."},{"metadata":{},"cell_type":"markdown","source":"We're going to write some functions here to make the entire model training process a little bit cleaner. We are also going to do some feature engineering. We will one-hot encode `Sex`, as well as create a new column called `Fsize`. `Fsize` (\"family size\") will contain a sum of the `Sibsp` and `Parch` columns for each row. Hopefully, this will elevate the robustness of our model training and translate to better cross-validated prediction performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    Run a cross-validated grid search and return the GridSearchCV object.\n    \n    Arguments:\n    classifier -- the model to be trained\n    param_grid -- the parameter grid on which to perform the search\n    X_train    -- feature training data\n    y_train    -- target training data\n    \n    Returns:\n    search -- GridSearchCV object\n\"\"\"\ndef pipe_train_cv(classifier, param_grid, X_train, y_train):\n    model_pipe = Pipeline(steps=[('StandardScaler', StandardScaler()), (str(classifier), classifier)])\n    \n    search = GridSearchCV(model_pipe, param_grid, n_jobs=-1, verbose=2)\n    search.fit(X_train, y_train)\n    \n    return (str(classifier), search)\n    \n\"\"\"\n    Get the best mean scores from a list of DataFrames containing GridSearchCV results.\n    \n    Arguments:\n    class_data_arr -- list of pandas DataFrames from GridSearchCV().cv_results_\n    name_arr       -- list of names corresponding to models in class_data_arr\n    \n    Returns:\n    best_scores_df -- DataFrame containing best mean score and best parameters for each model\n\"\"\"\ndef get_best_scores(class_data_arr, name_arr=None):\n    \n    if name_arr == None:\n        name_arr = [\"classifier{:02d}\".format(i) for i in range(len(class_data_arr))]\n    \n    class_data_arr_sorted = class_data_arr\n    \n    # sort the dataframes in the list\n    for i, df in enumerate(class_data_arr):\n        class_data_arr_sorted[i] = df.sort_values('rank_test_score').reset_index()\n\n    best_scores_df = pd.concat([pd.DataFrame([[df['mean_test_score'][0], df['params'][0]]],\n                                             columns = ['best_mean_score', 'best_param']) \n                                for df in class_data_arr_sorted], ignore_index=True)\n    \n    best_scores_df['classifier'] = name_arr\n    \n    return best_scores_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to one-hot encode the `Sex` column."},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_sex = pd.get_dummies(titanic_dropped_na_join.Sex, prefix = 'Sex')\ntitanic_dropped_na_join = titanic_dropped_na_join.join(one_hot_sex)\ntitanic_dropped_na_join","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to create the column `Fsize`, which will be a sum of `SibSp` and `Parch`. This represents size of family aboard the Titanic."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_dropped_na_join['Fsize'] = titanic_dropped_na_join.SibSp+titanic_dropped_na_join.Parch\ntitanic_dropped_na_join","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training models on the additionally feature-engineered dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping categorical data we won't be using in prediction models\nX_train = titanic_dropped_na_join.drop(columns=['Survived', 'Name', 'Ticket', 'Embarked', 'Sex'])\ny_train = np.ravel(titanic_dropped_na_join[['Survived']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search on MLPClassifier model\nmlp_param_grid = {'MLPClassifier()__hidden_layer_sizes' : [(10,), (10,10), (100,)],\n                  'MLPClassifier()__activation' : ['logistic', 'tanh', 'relu'],\n                  'MLPClassifier()__solver' : ['adam', 'sgd'],\n                  'MLPClassifier()__alpha' : [1e-7, 1e-5,1e-1, 1],\n                  'MLPClassifier()__learning_rate' : ['constant'], \n                  'MLPClassifier()__max_iter' : [1000] }\nmlp_name, mlp_search = pipe_train_cv(MLPClassifier(), mlp_param_grid, X_train, y_train)\n\n# Grid search on SupportVectorClassifier model\nsvc_param_grid = {'SVC()__C' : [1e-3,1e-1, 1.0, 10, 100],\n                  'SVC()__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n                  'SVC()__coef0' : [-1, -1e-2, 0.0, 1e-2, 1],\n                  'SVC()__degree' : [2,3,4],\n                  'SVC()__gamma' : ['scale', 'auto'] }\nsvc_name, svc_search = pipe_train_cv(SVC(), svc_param_grid, X_train, y_train)\n\n# Grid search on RandomForestClassifier model\nrfc_param_grid = {'RandomForestClassifier()__n_estimators' : [10, 100, 1000],\n                  'RandomForestClassifier()__max_depth' : [10, 50, 100, 1000],\n                  'RandomForestClassifier()__min_samples_split':[2,3,10],\n                  'RandomForestClassifier()__max_features' : ['auto', 'sqrt', 'log2', 0.5*X_train.shape[1]] }\nrfc_name, rfc_search = pipe_train_cv(RandomForestClassifier(), rfc_param_grid, X_train, y_train)\n\n# Grid search on KNeighborsClassifier model\nknc_param_grid = {'KNeighborsClassifier()__n_neighbors' : [2, 3, 5, 8, 10],\n                  'KNeighborsClassifier()__weights' : ['uniform', 'distance'],\n                  'KNeighborsClassifier()__p' : [1,2,3,4,5] }\n\nknc_name, knc_search = pipe_train_cv(KNeighborsClassifier(), knc_param_grid, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name_arr = [mlp_name, svc_name, rfc_name, knc_name]\nclass_data_arr = [\n                    pd.DataFrame(mlp_search.cv_results_),\n                    pd.DataFrame(svc_search.cv_results_),\n                    pd.DataFrame(rfc_search.cv_results_),\n                    pd.DataFrame(knc_search.cv_results_)\n                ]\n\nbest_scores_df = get_best_scores(class_data_arr, name_arr)\nbest_scores_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best mean cross-validated score from grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,8))\nsns.barplot(y='classifier', x='best_mean_score', data=best_scores_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performance increase from models trained on original dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_scores_df['best_mean_score_old'] = best_scores.best_mean_score\nbest_scores_df['perc_performance_increase'] = best_scores_df.apply(lambda row: abs(row.best_mean_score - row.best_mean_score_old)/row.best_mean_score_old*100, axis=1)\n\nfig = plt.figure(figsize=(16,8))\nsns.barplot(y='classifier', x='perc_performance_increase', data=best_scores_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting survivors on the Titanic\n\nFinally, we're going to apply our best estimator to predict the survivors in `test.csv`. We will use the SVC, although this model performed only marginally better than RandomForestClassifier. Perhaps with even more tweaking, or additionaly feature engineering, we might get even greater performance out of all our models."},{"metadata":{},"cell_type":"markdown","source":"We will need to perform the same feature-engineering on the testing set that we did on the training set. This includes one-hot encoding `Sex` and `Embarked`, and creating the column `Fsize`. We will not need to drop any NA values since we only wish to predict results, not train any models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the test data from test.csv\ntest_data = pd.read_csv(ROOT+DATADIR+'test.csv')\n\n# Need dummy column for Embarked_U because no Embarked NAs exist in testing set \ntest_data['Embarked_U'] = np.zeros(test_data.shape[0],dtype=int)\n\n# One hot encode Embarked\none_hot_embark = pd.get_dummies(test_data.Embarked, prefix = 'Embarked')\ntest_data_join = test_data.join(one_hot_embark)\n\n# One hot encode Sex\none_hot_sex = pd.get_dummies(test_data_join.Sex, prefix = 'Sex')\ntest_data_join = test_data_join.join(one_hot_sex)\n\n# Create Fsize\ntest_data_join['Fsize'] = test_data_join.SibSp+test_data_join.Parch\n\ntest_data_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_join.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During the descriptive analysis, we dropped a number of rows from the training data that contained NAs. We will not be able to do this with our testing set, so we need a strategy to fill NAs in `Age`, for example. We can ignore NAs in `Cabin` since we will not be using this column to make a prediction. \n\nLet's fill all NAs in `Age` with the average age. The motivation behind this strategy is this: if family size is somehow correlated with age, we hope that inserting the average age of all passengers will not substantially influence the correlation.\n\nThere is also a single `Fare` NA value. The strategy doesn't matter too much since there's only one value to replace. We will replace with the average."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_join['Age'] = test_data_join['Age'].fillna(test_data_join['Age'].mean())\ntest_data_join['Fare'] = test_data_join['Fare'].fillna(test_data_join['Fare'].mean())\ntest_data_join","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop irrelevant categorical columns from our testing set\nX_test = test_data_join.drop(columns=['Name', 'Ticket', 'Embarked', 'Sex', 'Cabin'])\nX_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop irrelevant categorical columns from our testing set\nX_test = test_data_join.drop(columns=['Name', 'Ticket', 'Embarked', 'Sex', 'Cabin'])\n\n# Get best SVC estimator from grid search results\nbest_svc = svc_search.best_estimator_\n\n# Get predictions\ny_pred = best_svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we construct a DataFrame from the predictions. We can save this DataFrame as a CSV and upload to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_df = pd.DataFrame(X_test['PassengerId'])\nprediction_df['Survived'] = y_pred\nprediction_df.to_csv('titanic_predictions.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nThe best cross-validated score we were able to achieve was 82% using a SVC model. This is reasonable, but a score greater than 90% is most desirable to be competitive. One major way we could probably increase performance is by including more rows of data in our training set; recall that we dropped all *rows* that contained NA values in `Cabin`. The motivation behind this step was that, if any correlation between `Survived` and `Cabin` (or any correlation between `Cabin` and a good predictor of `Survived`) exists, simply dropping the entire column of `Cabin` might remove this relationship entirely from our predictive models. However, any cost we pay for removing the `Cabin` column is probably less than the training performance we stand to gain from including an additional 127 observations. Such a study can be persued in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}